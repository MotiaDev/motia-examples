# Ollama Configuration - Required for AI responses
OLLAMA_HOST=http://localhost:11434
OLLAMA_MODEL=llama3.1

# Alternative models you can try (make sure they're installed in Ollama):
# OLLAMA_MODEL=llama3.1
# OLLAMA_MODEL=codellama
# OLLAMA_MODEL=mistral
# OLLAMA_MODEL=phi3